{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama run test : ollama pull qwen2:1.5b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 저는 유사하게 인간이 사용할 수 있는 인터페이스를 제공하는 AI 어시스턴트입니다. 어떤 도움을 드릴까요?\n"
     ]
    }
   ],
   "source": [
    "#local ollama inference\n",
    "\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='qwen2:1.5b-instruct', messages=[\n",
    "    {'role': 'system', 'content': '당신은 유용한 인공지능 어시스턴트입니다.'},\n",
    "    {'role': 'user', 'content': '안녕하세요. 자기소개 부탁합니다.'}\n",
    "])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrok 터널링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "#base_url은 잘 바뀌니, ngrok으로 터널링할 때마다 잘 바꿔줄 것 \n",
    "model = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0,\n",
    "                   base_url=\"https://d334-34-83-101-157.ngrok-free.app/\") \n",
    "\n",
    "response = model.invoke(\"안녕하세요. 자기소개를 부탁합니다. \")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능의 지도학습은 인공지능 모델을 학습시키는 과정입니다. 이 과정에서는 데이터를 사용하여 모델에 학습된 정보를 추가하고 이를 통해 모델이 더 잘 예측할 수 있는 능력을 향상시킵니다.\n",
      "\n",
      "예를 들어, 이미지 분류 모델은 이미지를 보고 그에 대한 정확한 클래스를 예측하는 데 도움을 줍니다. 이 과정에서는 많은 이미지 데이터가 필요하며, 이를 통해 모델이 이미지의 특징을 이해하고 그에 대한 정확한 예측을 할 수 있게 됩니다.\n",
      "\n",
      "또한, 지도학습은 인공지능 모델을 훈련시키는 데 사용되는 학습 알고리즘입니다. 이 알고리즘들은 데이터를 사용하여 모델의 가중치를 최적화하고 이를 통해 모델이 더 잘 예측할 수 있는 능력을 향상시킵니다.\n",
      "\n",
      "지도학습은 인공지능 분야에서 중요한 역할을 하고 있으며, 다양한 분야에서 활용되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "#local ollama inference\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "result = llm.invoke(\"인공지능의 지도학습이란?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}'))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'비지도 학습은 학습을 통해 새로운 정보를 배우는 과정에서, 학습자가 그 정보를 직접적으로 이해하고 적용하는 것을 의미합니다. 이는 일반적인 교육 시스템과 비슷한 방식으로 이루어집니다.\\n\\n예를 들어, 학생이 새로운 개념을 배우기 위해 문제를 풀고 그 결과를 분석하면, 그 학습자가 그 정보를 이해하고 적용하는 과정을 통해 새로운 정보를 학습합니다. 이는 비지도 학습과 동일한 효과를 가져옵니다.\\n\\n비지도 학습은 교육 시스템에서 사용되는 주요 기법 중 하나로, 학생들이 스스로 학습하고 문제 해결 능력을 향상시키기 위해 사용됩니다.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "result = chain.invoke({\"input\": \"비지도 학습이 무엇인가요?\"})\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP)는 인간이 말하는 언어를 이해하고, 이를 기반으로 정보를 추출하거나 생성하는 프로그래밍 기법입니다. 이 기술은 컴퓨터가 사용자로부터 입력을 받아서 그에 대한 답변을 제공하거나, 사용자가 질문을 하면 그에 대한 정확한 정보를 제공하는 데 사용됩니다.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "chain1.invoke({\"input\": \"자연어 처리란 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) is a programming methodology that allows humans to understand and extract information from the language they speak. This technology designs computers to interpret human language correctly. Generally, NLP includes Natural Language Processing (NLP), Artificial Intelligence (AI), and Machine Learning (ML) in various fields, enabling computers to effectively process the information humans communicate.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "#chain2\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"당신은 번역가입니다. 다음에 주어진 문장을 영어로 번역해주세요. {text}\"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = {\"text\":chain1} | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": \"자연어 처리란 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Natural Language Processing (NLP) ist ein Programmiertechnik, der menschliche Sprache verstehen und aufgrund dieser Informationen Informationen extrahieren oder erstellen kann. Diese Technologie wird Computer dazu gesteuert, menschliches Worten zu verstehen und reagieren. Normalerweise wird NLP in den Bereichen von Natur- und Künstliche Intelligenz (NLP), Maschinelles Lernen (Machine Learning) und anderen Bereichen verwendet, um Computers zur effizienten Verarbeitung menschlicher Informationen zu helfen.\"'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2\n",
    "lang = {\"lang\" : input(\"언어를 선택해주세요.\")}\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요. {text}\"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": \"자연어 처리란 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull qwen2:1.5b-instruct`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser()\n\u001b[0;32m     18\u001b[0m chain2 \u001b[38;5;241m=\u001b[39m RunnableMap({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnableLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: chain1), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnableLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: lang)}) \u001b[38;5;241m|\u001b[39m prompt2 \u001b[38;5;241m|\u001b[39m llm2 \u001b[38;5;241m|\u001b[39m output_parser\n\u001b[1;32m---> 19\u001b[0m \u001b[43mchain2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2493\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2489\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   2490\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2491\u001b[0m )\n\u001b[0;32m   2492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2493\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2495\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3140\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   3127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m   3128\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3129\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   3130\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3138\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3139\u001b[0m         ]\n\u001b[1;32m-> 3140\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   3141\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3961\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3959\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m   3960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 3961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3969\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3970\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3971\u001b[0m     )\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1596\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1593\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1594\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1595\u001b[0m         Output,\n\u001b[1;32m-> 1596\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1598\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1599\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1600\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1604\u001b[0m     )\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1606\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3845\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3841\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recursion_limit \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   3842\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRecursionError\u001b[39;00m(\n\u001b[0;32m   3843\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit reached when invoking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3844\u001b[0m         )\n\u001b[1;32m-> 3845\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3846\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3849\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3850\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecursion_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecursion_limit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3851\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3853\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Output, output)\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2495\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2494\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2495\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2496\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    169\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    460\u001b[0m ]\n\u001b[0;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m         )\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:276\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    254\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[0;32m    284\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[0;32m    285\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[0;32m    286\u001b[0m     )\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:207\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    200\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    205\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[0;32m    206\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:179\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    171\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m    172\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    175\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[0;32m    178\u001b[0m     }\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\github\\Ollama_TestCode\\venv\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:244\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[0;32m    245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code 404. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe your model is not found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand you should pull the model with `ollama pull \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m         )\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull qwen2:1.5b-instruct`."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요. {text}\"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) is a technology that allows computers to understand and interpret human language. This technology is widely used in various fields such as internet search, voice recognition, text message handling, etc.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"#Instruction \n",
    "                                           당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                           \n",
    "                                           {text}                                           \n",
    "                                           #Result                                           \n",
    "                                           \"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are Helpful AI BOT. Your name is 명지봇'),\n",
       " HumanMessage(content='이름이 뭐니?')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are Helpful AI BOT. Your name is 명지봇\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "output_parser = StrOutputParser()\n",
    "messages = chat_prompt.format_messages(user_input=\"이름이 뭐니?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는 명지봇입니다.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | output_parser\n",
    "chain.invoke({\"user_input\": \"네 이름이 뭐니?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatTemplate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful AI bot. Your name is 김철수.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='니 이름이 뭐니?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "\n",
    "template3 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"assistant\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "input_value =    {\n",
    "        \"name\": \"김철수\",\n",
    "        \"user_input\": \"니 이름이 뭐니?\"\n",
    "    }\n",
    "\n",
    "prompt_value = template3.invoke(input_value)\n",
    "output_parser = StrOutputParser()\n",
    "print(prompt_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'저는 김철수입니다. 어떻게 도와드릴까요?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3 = template3 | llm | output_parser\n",
    "response = chain3.invoke(input_value)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제해결 : 앞에서 만든 번역 Chain을 from_messages 메소드를 이용하여 인스트럭션을 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능이란?\n",
      "스페인어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) es un tipo de tecnología que permite a los ordenadores aprender y resolver problemas de la misma manera que hacen las personas. Este sistema puede analizar datos, crear información nueva o tomar decisiones basadas en esos datos. Esta tecnología se utiliza en diferentes campos, como el reconocimiento de voz, procesamiento de imágenes, y trabajos automáticos, entre otros.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_messages([(\"system\", \"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "                                            (\"user\",\"{input}\")\n",
    "                                            ])\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_messages([(\"system\", \"\"\"\n",
    "                                            # Instruction \n",
    "                                            당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                            \"\"\"),\n",
    "                                            (\"user\", \"\"\"\n",
    "                                            # Text\n",
    "                                            {text}                                           \n",
    "                                            # Result\"\"\")])\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "print(question)\n",
    "print(lang)\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template([(\"system\",\"당신은 인공지능 전문가입니다. 사용자의 질문에 답변해주세요. \"),\n",
    "(\"user\",\" {input} \")])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                           #Instruction\n",
    "                                           당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요. \n",
    "                                           \n",
    "                                           {text}\n",
    "                                           \n",
    "                                           당신은 결과를 말할 때, 원문과 번역결과를 반드시 함꼐 말해주세요.\n",
    "                                        \n",
    "                                           #Result\n",
    "                                        \n",
    "                                           번역문:\n",
    "                                           원문 : \n",
    "                                           \"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are Helpful AI BOT. Your name is 명지봇'),\n",
       " HumanMessage(content='자기소개를 부탁드립니다.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate,  HumanMessagePromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are Helpful AI BOT. Your name is 명지봇\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"자기소개를 부탁드립니다.\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 명지봇입니다. 저는 인공지능으로서 사용자의 질문에 답변하고 정보 제공을 도와드릴 수 있습니다. 어떤 도움이 필요하신가요?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are Helpful AI BOT. Your name is 명지봇\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = chat_prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"user_input\":\"자기소개를 부탁해요.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제해결 : Message 템플릿으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마이클 잭슨에 대해 설명해주세요\n",
      "영어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Michael Jackson was an American singer, songwriter, producer, television host, film director and musician. He was born in 1964 and is one of the most famous singers from the 1970s and 80s. His music includes various genres such as hip hop and pop rock.\\n\\nHe also wrote and composed for the musical \"Thriller\" which premiered in 1982, and his musical achievements and influence grew significantly after its release. His music includes songs like \"Beat It\", \"Billie Jean\", \"Thriller\" among others.\\n\\nHe also starred in the musical \"The Moon Song\" and it premiered in 1983. His musical achievements and influence are recognized as one of the most famous singers, and he was also recognized as a film actor.\\n\\nMichael Jackson died in 2009 and many people still respect him for his music\\'s impact and personal issues.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "                                            ])\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Instruction \n",
    "                                            당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                            \"\"\"),\n",
    "                                            HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Text\n",
    "                                            {text}                                           \n",
    "                                            # Result\"\"\")])\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "print(question)\n",
    "print(lang)\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능\n",
      "영어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI refers to the technology that imitates human ability'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b-instruct\", temperature=0.3, num_predict=50, stop= [\"\\n\"], base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "                                            ])\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "params = {\n",
    "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
    "    \"num_predict\": 10,          # 생성할 최대 토큰 수\n",
    "    \"stop\": [\"\\n\"]              # 정지 시퀀스 설정\n",
    "}\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b-instruct\", **params, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Instruction \n",
    "                                            당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                            \"\"\"),\n",
    "                                            HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Text\n",
    "                                            {text}                                           \n",
    "                                            # Result\"\"\")])\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "print(question)\n",
    "print(lang)\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메소드화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI is a programming method utilizing complex systems from computer science to improve the capability of human imagination and technological skills by providing ways to generate, analyze, predict, solve problems, and process vision. AI uses computers to understand and apply what humans depend on in terms of science technology, which helps in the actions and decisions of human beings.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chains(question, lang, model):\n",
    "    # Chain1\n",
    "    llm1 = ChatOllama(model=model, base_url=\"http://192.168.0.57:11434/\")\n",
    "    prompt1 = ChatPromptTemplate.from_messages([\n",
    "                SystemMessagePromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "                HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "                                                ])\n",
    "    output_parser = StrOutputParser()\n",
    "    chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "    # Chain2\n",
    "    llm2 = ChatOllama(model=model, base_url=\"http://192.168.0.57:11434/\")\n",
    "    prompt2 = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "                                                # Instruction \n",
    "                                                당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                                \"\"\"),\n",
    "                                                HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "                                                # Text\n",
    "                                                {text}                                           \n",
    "                                                # Result\"\"\")])\n",
    "    chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1.invoke({\"input\": question})), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "    \n",
    "    # Run chain2\n",
    "    result = chain2.invoke({\"input\": question})\n",
    "    return result\n",
    "\n",
    "run_chains(\"인공지능이란?\", \"영어\", \"qwen2:1.5b-instruct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
